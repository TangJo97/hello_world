{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info-H-600 group work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Metadata information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports go here\n",
    "import os \n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import math\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose between Cluster or Local mode (write local or cluster in the field): local\n",
      "\n",
      "You have chosen local mode!\n"
     ]
    }
   ],
   "source": [
    "def choose_mode():\n",
    "    \"\"\"\n",
    "    Write \"cluster\" to set up in cluster mode or in \"local\" to run the notebook locally\n",
    "    \"\"\"\n",
    "    mode = input(\"Choose between Cluster or Local mode (write local or cluster in the field): \")\n",
    "    \n",
    "    \n",
    "    if mode.lower() == \"local\":\n",
    "        basedir = \"./data/sampled\"\n",
    "        print(\"\\nYou have chosen local mode!\")\n",
    "        return(basedir)\n",
    "        \n",
    "        \n",
    "    elif mode.lower() == \"cluster\":\n",
    "        basedir = '/home/epb199/data/'\n",
    "        print(\"\\nYou have chosen cluster mode!\")\n",
    "        return(basedir)\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nError, incorrect entries, select between \\\"local\\\" or \\\"cluster\\\" !\")\n",
    "\n",
    "        \n",
    "        \n",
    "basedir = choose_mode()\n",
    "\n",
    "\n",
    "\n",
    "if basedir == '/home/epb199/data/':\n",
    "    \n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=3g  pyspark-shell\"\n",
    "\n",
    "    os.environ['HADOOP_CONF_DIR']=\"/etc/hadoop/conf\"\n",
    "\n",
    "    os.environ['PYSPARK_PYTHON']=\"/usr/local/anaconda3/bin/python\"\n",
    "    os.environ['PYSPARK_DRIVER_PYTHON']=\"/usr/local/anaconda3/bin/python\"\n",
    "\n",
    "\n",
    "    try: \n",
    "        spark\n",
    "        print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "        spark.stop()\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\") \\\n",
    "        .config(\"spark.executor.instances\",\"4\") \\\n",
    "        .appName(\"demoRDD\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "    sc=spark.sparkContext    \n",
    "    \n",
    "elif basedir == './data/sampled':\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=4g  pyspark-shell\"\n",
    "\n",
    "    try: \n",
    "        spark\n",
    "        print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "        spark.stop()\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"demoRDD\") \\\n",
    "        .getOrCreate()\n",
    "    sc=spark.sparkContext    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 general statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.21 Function to get general statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_all_sub_data(sub_data, basedir = basedir):\n",
    "    \"\"\"\n",
    "    This function computes the meta-data information about a defined sub-database (i.e. yellow data)\n",
    "    @sub_data: This parameter must be a string specifying which sub-data we are interested by (i.e. 'yellow')\n",
    "    @basedir: This parameter specifies where the data folder is. It must be a string.\n",
    "    \"\"\"\n",
    "    start_name = sub_data + \"_\"\n",
    "    names = os.listdir(basedir)\n",
    "    paths = [basedir +\"/\"+name for name in names if name.startswith(start_name)]\n",
    "    nbr_files = len(paths)\n",
    "    sizes = [os.stat(path).st_size for path in paths]\n",
    "    lengths = [sc.textFile(path).count() for path in paths]\n",
    "    print(\"*************************information about the\", sub_data, \"files*************************\", \n",
    "          \"\\nNumber of files\", \"files:\", \n",
    "          nbr_files,\n",
    "          \"\\n\\n**************size information about\", sub_data, \"files in bytes\" + \"***********\",\n",
    "          \"\\nmean:\" ,np.mean(sizes),\n",
    "          \"\\nmin:\", min(sizes),\n",
    "          \"\\nmax:\", max(sizes),\n",
    "          \"\\n 25th quantile:\", np.percentile(sizes,25),\n",
    "          \"\\n50th quantile:\", np.percentile(sizes,50),\n",
    "          \"\\n75th quantile:\", np.percentile(sizes,75),\n",
    "          \"\\n90th quantile:\", np.percentile(sizes,90),\n",
    "          \"\\n\\n**************Number of lines in\", sub_data, \n",
    "          \"files\" + \"**************\",\n",
    "          \"\\nmean:\" ,np.mean(lengths),\n",
    "          \"\\nmin:\", min(lengths),\n",
    "          \"\\nmax:\", max(lengths),\n",
    "          \"\\n25th quantile:\", np.percentile(lengths,25),\n",
    "          \"\\n50th quantile:\", np.percentile(lengths,50),\n",
    "          \"\\n75th quantile:\", np.percentile(lengths,75),\n",
    "          \"\\n90th quantile:\", np.percentile(lengths,90),\n",
    "          \"\\n\",\"\\n\",\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.22 Metadata statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************information about the yellow files************************* \n",
      "Number of files files: 131 \n",
      "\n",
      "**************size information about yellow files in bytes*********** \n",
      "mean: 3750759.6870229007 \n",
      "min: 43103 \n",
      "max: 5959352 \n",
      " 25th quantile: 1756967.0 \n",
      "50th quantile: 4442047.0 \n",
      "75th quantile: 5123591.5 \n",
      "90th quantile: 5491438.0 \n",
      "\n",
      "**************Number of lines in yellow files************** \n",
      "mean: 24204.51145038168 \n",
      "min: 477 \n",
      "max: 32301 \n",
      "25th quantile: 19990.0 \n",
      "50th quantile: 26295.0 \n",
      "75th quantile: 29081.0 \n",
      "90th quantile: 30203.0 \n",
      " \n",
      " \n",
      "\n",
      "*************************information about the green files************************* \n",
      "Number of files files: 76 \n",
      "\n",
      "**************size information about green files in bytes*********** \n",
      "mean: 262437.2894736842 \n",
      "min: 2512 \n",
      "max: 570765 \n",
      " 25th quantile: 121494.75 \n",
      "50th quantile: 190194.5 \n",
      "75th quantile: 456955.0 \n",
      "90th quantile: 499751.0 \n",
      "\n",
      "**************Number of lines in green files************** \n",
      "mean: 2027.5 \n",
      "min: 16 \n",
      "max: 3547 \n",
      "25th quantile: 1359.75 \n",
      "50th quantile: 2073.5 \n",
      "75th quantile: 2893.25 \n",
      "90th quantile: 3127.0 \n",
      " \n",
      " \n",
      "\n",
      "*************************information about the fhv files************************* \n",
      "Number of files files: 64 \n",
      "\n",
      "**************size information about fhv files in bytes*********** \n",
      "mean: 1147257.84375 \n",
      "min: 52060 \n",
      "max: 3339455 \n",
      " 25th quantile: 218111.25 \n",
      "50th quantile: 646615.0 \n",
      "75th quantile: 2257156.5 \n",
      "90th quantile: 3020144.5 \n",
      "\n",
      "**************Number of lines in fhv files************** \n",
      "mean: 21713.625 \n",
      "min: 960 \n",
      "max: 47673 \n",
      "25th quantile: 6031.5 \n",
      "50th quantile: 21693.0 \n",
      "75th quantile: 33845.75 \n",
      "90th quantile: 43709.9 \n",
      " \n",
      " \n",
      "\n",
      "*************************information about the fhvhv files************************* \n",
      "Number of files files: 10 \n",
      "\n",
      "**************size information about fhvhv files in bytes*********** \n",
      "mean: 2007750.7 \n",
      "min: 535789 \n",
      "max: 2978931 \n",
      " 25th quantile: 1121047.25 \n",
      "50th quantile: 2542280.0 \n",
      "75th quantile: 2687857.25 \n",
      "90th quantile: 2804332.8 \n",
      "\n",
      "**************Number of lines in fhvhv files************** \n",
      "mean: 32182.9 \n",
      "min: 8626 \n",
      "max: 47704 \n",
      "25th quantile: 18023.0 \n",
      "50th quantile: 40715.0 \n",
      "75th quantile: 43057.25 \n",
      "90th quantile: 44923.0 \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "info_all_sub_data(\"yellow\")\n",
    "info_all_sub_data(\"green\")\n",
    "info_all_sub_data(\"fhv\")\n",
    "info_all_sub_data(\"fhvhv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Analysis of the schema evolution.\n",
    "\n",
    "Over time, the relational schema associated to each type of trip data (yellow, green, fhv, hvfhv) has changed. Let us analyze the changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.31 Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to help analyze the schema changes goes here\n",
    "import os \n",
    "\n",
    "def get_schemas(sub_data, basedir = basedir):\n",
    "    \"\"\"It outputs the list of tuples which are the month and the schema for a given files in a subdatabase\"\"\"\n",
    "    start_name = sub_data + \"_\"\n",
    "    names = os.listdir(basedir)\n",
    "    paths = [basedir +\"/\"+name for name in names if name.startswith(start_name)]\n",
    "    list_of_schemas = []\n",
    "    for path in paths:\n",
    "        x = open(path)\n",
    "        list_of_schemas.append((path[-11: -4], x.readlines()[0].strip().lower()))\n",
    "    return(sorted(list_of_schemas))\n",
    "\n",
    "\n",
    "def unique_schema(list_of_schemas):\n",
    "    \"\"\" Compute the unique schemas of a sub-dataset along with the first date where it changed\n",
    "    The only input is a list of multiple schemas along with their dates (see get_schemas)\"\"\"\n",
    "    schemas = list_of_schemas\n",
    "    list_of_unique_schema = [schemas[0]]\n",
    "    for date, schema in schemas:\n",
    "        if schema.replace(\" \", \"\") != list_of_unique_schema[-1][1]: #important to remove whitespaces\n",
    "            list_of_unique_schema.append((date, schema))\n",
    "    return(list_of_unique_schema)\n",
    "\n",
    "\n",
    "def diff_schemas(sub_data, basedir = basedir):\n",
    "    \"\"\"This function returns the different schemas over a period and subdatabase. The date indicates the first\n",
    "    time that this schema was used\n",
    "    The only input is the name of the directory that contains the subdatabase\"\"\"\n",
    "    schemas = get_schemas(sub_data, basedir)\n",
    "    different_schemas = unique_schema(schemas)\n",
    "    print(\"There have been\", len(different_schemas)-1, \"changes in this subdatabase\")\n",
    "    return(different_schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.32 Analysis of schema changes for fhv cab data files\n",
    "\n",
    "Analyze the schema changes for the FHV cab data files. Write down your conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_all_schemas = get_schemas('fhv')\n",
    "unique_schema_fhv = diff_schemas(\"fhv\")\n",
    "print(unique_schema_fhv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changes to `fhv` are the following:\n",
    "- From January 2015 to December 2016, the database had only 3 columns: the `dispatching base number` which is the TLC Base License number of the base that provided the ride. The `pickup_date`which is the date and time of a given trip and `the location ID` which is the TLC taxi zone where the trip began\n",
    "- From January 2017 to June 2017, they made some changes & added some columns: they changed `pickup_date` to `pickup_datetime` which is just a change of name, they added a `drop_off_datetime`. They added also the `pulocationid` and the `dolocationid`which are the TLC taxi zone in which the trip began and ended to replace the `locationid`. \n",
    "- From July 2017 to December 2017, they added the column `sr_flag` which denotes if the trip was done by a shared ride chain. Note that for Lyft, it does not necessarly indicates that the trip was shared because they also recorded as \"1\" even if the trip was not shared in the end.\n",
    "- From January 2018 to December 2018, they only sent to the end of the columns the `dispatching_base_number` and added a `dispatching_base_num` but this column is a blank one and cannot really count as a column.\n",
    "- From January 2019 to now, they removed the blank column `dispatching_base_num` and moved back the `dispatching base_number` to the start of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.33 Analysis of schema changes for fhvhv data files\n",
    "\n",
    "Analyze the schema changes for the FHV cab data files. Write down your conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_schemas(\"fhvhv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is pretty recent (started in february 2019) and has not undergo changes. <br>\n",
    "The colums are the same as `fhv` (the schema of 2019), except that it has a supplementary column at the start `hvfhs_license_num` which gives the information on which companies provided the ride and also the number of the car that provided it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.34 Analysis of schema changes for green cab data files\n",
    "\n",
    "Analyze the schema changes for the green taxi data files. Write down your conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There have been 3 changes in this subdatabase\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2013-08',\n",
       "  'vendorid,lpep_pickup_datetime,lpep_dropoff_datetime,store_and_fwd_flag,ratecodeid,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,total_amount,payment_type,trip_type'),\n",
       " ('2015-01',\n",
       "  'vendorid,lpep_pickup_datetime,lpep_dropoff_datetime,store_and_fwd_flag,ratecodeid,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type'),\n",
       " ('2016-07',\n",
       "  'vendorid,lpep_pickup_datetime,lpep_dropoff_datetime,store_and_fwd_flag,ratecodeid,pulocationid,dolocationid,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type'),\n",
       " ('2019-01',\n",
       "  'vendorid,lpep_pickup_datetime,lpep_dropoff_datetime,store_and_fwd_flag,ratecodeid,pulocationid,dolocationid,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type,congestion_surcharge')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_schemas(\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Green database is the second largest and underwent 3 changes of schemas:\n",
    "- From August 2013 to December to 2014, its first schema was the one used at the same as the schema of yellow between January 2015 to June 2016, at the exceptions of  `vendor_id` was here `vendorid`, `pickup_datetime` was `lpep_pickup_datetime`, `dropoff_datetime` was `lpep_dropoff_datetime`. There was also two supplementary variables, namely `ehail_fee`, which is a blank column (I am not sure though!!!!!!!!) and finally `trip_type` which is 1 if the trip was a street hail or 2 if it was a dispatch.\n",
    "- From January 2015 to June 2016, the green database underwent the same process as yellow one. Indeed the latitude and longitude columns were replaced by `pulocationid`, `dolocationid`.\n",
    "- In January 2019, the last change happened to the database where a new column was added, the `congestion_surchage` (same as yellow database)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.35 Analysis of schema changes for yellow cab data files\n",
    "\n",
    "Analyze the schema changes for the Yellow taxi data files. Write down your conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_schemas(\"yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `yellow` database is the oldest and underwent several changes:\n",
    "- From January 2009 to December 2009, they used `the vendor_name` which is the name of the data_provider (either CMT or LLC). Then, they have the columns `trip_pickup_datetime`, `trip_dropoff_date_time`, `passenger_count` and `trip_distance` which are all self-explanatory. `start_lon` and `start_lat` are respectively the longitute and latitude of the pick-up. Then, there was the rate code which is an integer representing the last fare/rate applied to the trip. The variable `store_and_forward` stores wheter the vehicule downloaded the record because it was offline or not (so it can sent it later). `end_lon` and `end_lat` are the longitude and latitude of the drop-oof. `payment_type` specified how the passenger paid for the trip. `fare_amt` gives the time-and-distance fare in dollar per meter. `surcharge` represent the eventual surchages for rush hour or overnight;  `mta_tax` is the amount of a tax that is automatically triggered when the trip is longer than a given distance. `tip_amt` is the amount of tip (when paid by card, cash tips are not recorded). `tolls_amt` gives the amount of tolls paid and  `total_amt` is the total amount paid for this trip. \n",
    "- From January 2010 to December 2014, they changed nearly all the names to make them more explicit, but the order and the meaning of the data.  <br>\n",
    "`the vendor_name` &#8594; `vendor_id` which is now an integer & not a string anymore; `trip_pickup_datetime` &#8594; `pickup_datetime` ; `trip_dropoff_date_time` &#8594; `dropoff_datetime` ; `start_lon` &#8594; `pickup_longitude` ; `start_lat` &#8594; `pickup_latitude`; `store_and_forward` &#8594; `store_and_fwd_flag`; `end_lon` &#8594; `dropoff_longitude` ; `end_lat` &#8594; `dropoff_latitude` ; `fare_amt` &#8594; `fare_amount`; `tip_amt` &#8594; `tip_amount`; `tolls_amt` &#8594; `tolls_amount`; `total_amt` &#8594; `total_amount`\n",
    "- From January 2015 to June 2016, the main changes were renaming. `vendor_id` became `vendorid`, `pickup_datetime` and `dropoff_datetime`became `tpep_pickup_datetime` and `tpep_dropoff_datetime`. `rate_code` was also changed to `ratecodeid`. `surchage` was renamed into `extra` to include a new variable `improvement_surchage` which is an extra amount depending on the dropoff location which started in 2015. This new variable was placed between `tolls_amount` and `total_amount`.\n",
    "- From July 2016 to December 2018, the two variables `pickup_longitude` and `pickup_latitude`  were replaced by a single variable `pulocationid` which reprensent the TLC taxi zone of the pickup. The same happened to `dropoff_longitude` and `dropoff_latitude` which were replaced by `dolocationid`. They wer both placed after `store_and_forward_flag`.\n",
    "- From January 2019 to now, it was added the variable `congestion surcharge` which is a new surchage if there is a congestion. \n",
    "\n",
    "\n",
    "<u> Side note: </u> in the year 2014, the columns of the data were encoded in this manner: vendor_id, trip_pickup_datetime, with a whitespace after the \",\", which is the details that might have its importance in the merging section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sub-dataset, write (and execute) code that converts a file (using possibly an old schema) into a file that has the new, latest schema version.\n",
    "\n",
    "Your conversion code should not modify the original files, but instead create a new file. 2\n",
    "\n",
    "Be sure to explain the design behind your conversion functions!\n",
    "\n",
    "The data integration step is highly parallellizable. Therefore, your solution on this part\n",
    "**must** be written in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"2,2013-08-24 22:35:15,2013-08-24 22:45:34,N,1,-73.844230651855469,40.721363067626953,-73.86566162109375,40.720394134521484,3,2.47,10,0.5,0.5,0,0,,11,2\".split(\",\")\n",
    "test1 = \"2,2013-08-31 23:11:03,2013-08-31 23:19:57,N,1,-73.847312927246094,40.754188537597656,-73.857452392578125,40.733768463134766,1,2.75,10.5,0.5,0.5,0,0,,11.5,2\".split(\",\")\n",
    "test2 = [test, test1]\n",
    "test3 = sc.parallelize(test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[94, 92]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3.map(lambda x: transfo_lon_lat(x[5], x[6])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = gpd.read_file('metadata/taxi_zones.shp')\n",
    "zones = zones.to_crs({'init':'epsg:4326'})\n",
    "rtree = zones.sindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtree = zones.sindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Auxillary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = gpd.read_file('metadata/taxi_zones.shp')\n",
    "zones = zones.to_crs({'init':'epsg:4326'})\n",
    "\n",
    "def transfo_lon_lat(lon, lat, zones = zones):\n",
    "    \"\"\"\n",
    "    This function computes from a list (or a row in a RDD) that specifies the latitude & longitude of a point\n",
    "    and returns the index of the geographic zone that contains it.\n",
    "    @lon: it represents the longitude of a given point\n",
    "    @lat: it represents the latitude of a given point\n",
    "    \"\"\"\n",
    "    rtree = zones.sindex\n",
    "    \n",
    "    try: \n",
    "        point = Point(float(lon), float(lat))\n",
    "        psb_zones = list(rtree.intersection(point.bounds))\n",
    "        out = [zone for zone in psb_zones if zones.iloc[zone].geometry.contains(point)]\n",
    "        if out == []:\n",
    "            return(\"\") #if not in any zone\n",
    "        elif len(out) >1:\n",
    "            return(out[0]) #if there are multiple zones matching the given point (on a border for example)\n",
    "        #it selects the first one.\n",
    "        else:\n",
    "            return(out[0])\n",
    "    except:\n",
    "        return(\"\") #if the input cannot be converted to a float\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def separating_schema(sub_data, unique_schema, basedir = basedir):\n",
    "    \"\"\"This function returns a list where each element of the list is the input which will be able to separate to\n",
    "    select the various files that have the same schemas together. Therefore, the output is a list for which the \n",
    "    number of element is the number of different schemas for a given datatabase\n",
    "    The first input is the name of the directory which contains the data about a given subfiles\n",
    "    The second input is the output of the function unique_schema on the given databases\"\"\"\n",
    "    start_name = sub_data + \"_\"\n",
    "    #basedir =  \"./home/epb199/data/\"\n",
    "    names = os.listdir(basedir)\n",
    "    paths = [basedir +\"/\"+name for name in names if name.startswith(start_name)]\n",
    "    list_ofpaths = []\n",
    "    for path in paths:\n",
    "        if path != './data/'+ sub_data + '/.ipynb_checkpoints':     #j'ai simplifié ici\n",
    "            #x = open(path)\n",
    "            list_ofpaths.append((path[-11: -4], path))\n",
    "   \n",
    "    separating_list = []\n",
    "    list_paths = sc.parallelize(sorted(list_ofpaths)).map(lambda x: (dt.strptime(x[0],'%Y-%m'), x[1]))\n",
    "    \n",
    "    for sche_idx in range(1,len(unique_schema)):\n",
    "        list_paths1 = list_paths.filter(lambda x: dt.strptime(unique_schema[sche_idx - 1][0],'%Y-%m' ) <= x[0] <  dt.strptime(unique_schema[sche_idx][0],'%Y-%m' ))\n",
    "        list_paths1 = list_paths1.map(lambda x : x[1]).reduce(lambda x, y: x+\",\"+y)\n",
    "        separating_list.append(list_paths1)\n",
    "    \n",
    "    list_paths2 = list_paths.filter(lambda x: dt.strptime(unique_schema[len(unique_schema) - 1][0],'%Y-%m' ) <= x[0] )\n",
    "    list_paths2 = list_paths2.map(lambda x : x[1]).reduce(lambda x, y: x+\",\"+y)\n",
    "    separating_list.append(list_paths2)\n",
    "    return(separating_list)\n",
    "\n",
    "\n",
    "\n",
    "def unified_sub_schema(sub_data, unique_schema, idx, basedir):\n",
    "    \"\"\"This function returns a unified RDD with  data that shares the same schema.\n",
    "    The two inputs are the name of the directory with the subdata & the index that correspond to the number of the\n",
    "    schema (i.e. the first schema putted in place between 2010 & 2013 will have an index of 0, the following schema\n",
    "    will have the index 1)\"\"\"\n",
    "    schema = separating_schema(sub_data, unique_schema, basedir )[idx]\n",
    "    RDD_unified =  sc.textFile(schema)\n",
    "    RDD_clean = RDD_unified.filter(lambda x: (x.replace(\" \", \"\").lower() != unique_schema[idx][1])) #remove the column names\n",
    "    return(RDD_clean)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def flatten_nan(x):\n",
    "    \"\"\"This function allows to flatten a list of list & encode missing values\"\"\"\n",
    "    z = [val.upper() for sublist in x for val in sublist]\n",
    "    #w = [None if (val == \"\") else val for val in z]\n",
    "    return(z)\n",
    "\n",
    "\n",
    "\n",
    "def sr_flag_switch(x):\n",
    "    \"\"\"\n",
    "    This function changes the shared flag variable into a binary variable (== 1 if ride was shared 0 if not)\n",
    "    In the data dictionnary, the ride recorded with \"\" means that the ride was not shared, so we just switch\n",
    "    to a binary variable to diffentiate with the recording when the shared_flag was missing.\n",
    "    \"\"\"\n",
    "    if x == \"1\":\n",
    "        return(x)\n",
    "    elif (x == \"0\") or (x == \"\"):\n",
    "        return(\"0\")\n",
    "    else:\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.3 FHV integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_all_schemas = get_schemas('fhv') \n",
    "unique_schema_fhv = diff_schemas(\"fhv\")\n",
    "print(unique_schema_fhv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modifying & joining the various schema\n",
    "\n",
    "\n",
    "fhv0 = (unified_sub_schema(\"fhv\",unique_schema_fhv,  0, basedir)\n",
    "        .map(lambda x : x.replace(\"\\\"\", \"\").split(\",\"))\n",
    "        .map(lambda x : [x[0].upper(), x[1].upper(), \"\", x[2].upper(), \"\", \"\"]))\n",
    "\n",
    "\n",
    "fhv1 = (unified_sub_schema(\"fhv\",unique_schema_fhv,  1, basedir)\n",
    "        .map(lambda x : x.replace(\"\\\"\", \"\").split(\",\"))\n",
    "        .map(lambda x : flatten_nan([x[0:5], [\"\"]])))\n",
    "\n",
    "\n",
    "fhv2 = (unified_sub_schema(\"fhv\",unique_schema_fhv,  2, basedir)\n",
    "        .map(lambda x : x.replace(\"\\\"\", \"\").split(\",\"))\n",
    "        .map(lambda x : flatten_nan([x[:5], [sr_flag_switch(x[5])]])))\n",
    "\n",
    "\n",
    "fhv3 = (unified_sub_schema(\"fhv\",unique_schema_fhv, 3, basedir)\n",
    "        .map(lambda x : x.replace(\"\\\"\", \"\").split(\",\"))\n",
    "        .map(lambda x : flatten_nan([[x[5]], x[0:4], [sr_flag_switch(x[4])]]) ))\n",
    "\n",
    "\n",
    "fhv4 = (unified_sub_schema(\"fhv\",unique_schema_fhv,  4, basedir)\n",
    "        .map(lambda x : x.split(\",\"))\n",
    "        .map(lambda x : flatten_nan([x[:5], [sr_flag_switch(x[5])]] )))\n",
    "        \n",
    "        \n",
    "\n",
    "fhv_data = fhv0.union(fhv1).union(fhv2).union(fhv3).union(fhv4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_data.sample(False, 0.0003).collect() #first look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 FHVHV Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There have been 0 changes in this subdatabase\n",
      "[('2019-02', 'hvfhs_license_num,dispatching_base_num,pickup_datetime,dropoff_datetime,pulocationid,dolocationid,sr_flag')]\n"
     ]
    }
   ],
   "source": [
    "#loading the different schemas\n",
    "\n",
    "fhvhv_all_schemas = get_schemas('fhvhv') \n",
    "unique_schema_fhvhv = diff_schemas(\"fhvhv\")\n",
    "print(unique_schema_fhvhv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhvhv_data = (unified_sub_schema(\"fhvhv\",unique_schema_fhvhv,  0, basedir)\n",
    "             .map(lambda x : x.replace(\"\\\"\", \"\").split(\",\")).\n",
    "              map(lambda x: flatten_nan([x[:6], [sr_flag_switch(x[6])]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhvhv_data.sample(False, 0.0003).collect() #first look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Green Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There have been 3 changes in this subdatabase\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2013-08',\n",
       "  'vendorid,lpep_pickup_datetime,lpep_dropoff_datetime,store_and_fwd_flag,ratecodeid,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,total_amount,payment_type,trip_type'),\n",
       " ('2015-01',\n",
       "  'vendorid,lpep_pickup_datetime,lpep_dropoff_datetime,store_and_fwd_flag,ratecodeid,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type'),\n",
       " ('2016-07',\n",
       "  'vendorid,lpep_pickup_datetime,lpep_dropoff_datetime,store_and_fwd_flag,ratecodeid,pulocationid,dolocationid,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type'),\n",
       " ('2019-01',\n",
       "  'vendorid,lpep_pickup_datetime,lpep_dropoff_datetime,store_and_fwd_flag,ratecodeid,pulocationid,dolocationid,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type,congestion_surcharge')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the different schemas\n",
    "\n",
    "green_all_schemas = get_schemas('green') \n",
    "unique_schema_green = diff_schemas(\"green\")\n",
    "unique_schema_green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modifying & joining the various schema\n",
    "\n",
    "green0 = (unified_sub_schema(\"green\",unique_schema_green,  0, basedir)\n",
    "        .map(lambda x : x.replace(\"\\\"\", \"\").split(\",\"))\n",
    "        .map(lambda x : flatten_nan([x[0:5], [str(transfo_lon_lat(x[5], x[6]))], [str(transfo_lon_lat(x[7],x[8]))],\n",
    "                        x[9:17],[\"0\"], x[17:20], [\"0\"]])))\n",
    "\n",
    "\n",
    "green1 = (unified_sub_schema(\"green\",unique_schema_green,  1, basedir)\n",
    "        .map(lambda x : x.replace(\"\\\"\", \"\").split(\",\"))\n",
    "        .map(lambda x : flatten_nan([x[0:5], [str(transfo_lon_lat(x[5], x[6]))], [str(transfo_lon_lat(x[7],x[8]))],\n",
    "                        x[9:21], [\"0\"]])))\n",
    "\n",
    "\n",
    "green2 = (unified_sub_schema(\"green\",unique_schema_green,  2, basedir)\n",
    "        .map(lambda x : x.replace(\"\\\"\", \"\").split(\",\"))\n",
    "        .map(lambda x : flatten_nan([x[0:19], [\"0\"]])))\n",
    "\n",
    "\n",
    "green3 = (unified_sub_schema(\"green\",unique_schema_green, 3, basedir)\n",
    "        .map(lambda x : x.replace(\"\\\"\", \"\").split(\",\"))\n",
    "        .map(lambda x : flatten_nan([x[0:20]])))\n",
    "\n",
    "green_data = green0.union(green1).union(green2).union(green3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_data.sample(False, 0.0003).collect() #first look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yellow integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There have been 4 changes in this subdatabase\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2009-01',\n",
       "  'vendor_name,trip_pickup_datetime,trip_dropoff_datetime,passenger_count,trip_distance,start_lon,start_lat,rate_code,store_and_forward,end_lon,end_lat,payment_type,fare_amt,surcharge,mta_tax,tip_amt,tolls_amt,total_amt'),\n",
       " ('2010-01',\n",
       "  'vendor_id,pickup_datetime,dropoff_datetime,passenger_count,trip_distance,pickup_longitude,pickup_latitude,rate_code,store_and_fwd_flag,dropoff_longitude,dropoff_latitude,payment_type,fare_amount,surcharge,mta_tax,tip_amount,tolls_amount,total_amount'),\n",
       " ('2015-01',\n",
       "  'vendorid,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,pickup_longitude,pickup_latitude,ratecodeid,store_and_fwd_flag,dropoff_longitude,dropoff_latitude,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount'),\n",
       " ('2016-07',\n",
       "  'vendorid,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,ratecodeid,store_and_fwd_flag,pulocationid,dolocationid,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount'),\n",
       " ('2019-01',\n",
       "  'vendorid,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,ratecodeid,store_and_fwd_flag,pulocationid,dolocationid,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount,congestion_surcharge')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the different schemas\n",
    "\n",
    "yellow_all_schemas = get_schemas('yellow') \n",
    "unique_schema_yellow = diff_schemas(\"yellow\")\n",
    "unique_schema_yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_val_list = lambda x: [val.strip() for val in x] #because one period had whitespace in front of every\n",
    "#values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow0 = (unified_sub_schema(\"yellow\",unique_schema_yellow,  0, basedir)\n",
    "           .map(lambda x : strip_val_list( x.replace(\"\\\"\", \"\").split(\",\"))) \n",
    "           .map(lambda x : flatten_nan([x[:5], x[7:9], [str(transfo_lon_lat(x[5],x[6]))], \n",
    "                                        [str(transfo_lon_lat(x[9],x[10]))], x[11:17], [\"0\"], [x[17]], [\"0\"]])))\n",
    "\n",
    "\n",
    "                \n",
    "#[x[0], x[1], x[2], x[3], x[4], x[7], x[8], str(transfo_lon_lat(x[5],x[6])), str(transfo_lon_lat(x[9],x[10])), x[11], x[12], x[13], x[14],  x[15], x[16],'0', x[17], '0']))\n",
    "\n",
    "yellow1 = (unified_sub_schema(\"yellow\",unique_schema_yellow,  1, basedir)\n",
    "           .map(lambda x : strip_val_list( x.replace(\"\\\"\", \"\").split(\",\")))\n",
    "           .map(lambda x : flatten_nan([x[:5], x[7:9], [str(transfo_lon_lat(x[5],x[6]))], \n",
    "                                        [str(transfo_lon_lat(x[9],x[10]))], x[11:17], [\"0\"], [x[17]], [\"0\"]])))\n",
    "\n",
    "\n",
    "\n",
    "yellow2 = (unified_sub_schema(\"yellow\",unique_schema_yellow,  2, basedir)\n",
    "           .map(lambda x : strip_val_list( x.replace(\"\\\"\", \"\").split(\",\")))\n",
    "           .map(lambda x : flatten_nan([x[:5], x[7:9], [str(transfo_lon_lat(x[5],x[6]))], \n",
    "                            [str(transfo_lon_lat(x[9],x[10]))], x[11:19], [\"0\"]]))) #I removed the x[19], he was out of range\n",
    "\n",
    "\n",
    "\n",
    "#[x[0], x[1], x[2], x[3], x[4], x[7], x[8], str(transfo_lon_lat(x[5],x[6])), str(transfo_lon_lat(x[9],x[10])), x[11], x[12], x[13], x[14], x[15], x[16], x[17], x[18], '0']\n",
    "\n",
    "\n",
    "yellow3 = (unified_sub_schema(\"yellow\",unique_schema_yellow, 3, basedir)\n",
    "           .map(lambda x : strip_val_list( x.replace(\"\\\"\", \"\").split(\",\")))\n",
    "           .map(lambda x : flatten_nan([x[:17], [\"0\"]]))) # I completely changed bc there was no more lat & lon variable in that schema\n",
    "#it should go to 17 no?\n",
    "\n",
    "\n",
    "yellow4 = (unified_sub_schema(\"yellow\",unique_schema_yellow, 4, basedir) #I replaced the 3 by 4 here\n",
    "           .map(lambda x : strip_val_list( x.replace(\"\\\"\", \"\").split(\",\")))\n",
    "           .map(lambda x : flatten_nan([x[ :18]]))) \n",
    "\n",
    "\n",
    "yellow_data = yellow0.union(yellow1).union(yellow2).union(yellow3).union(yellow4) #I added the .union(yellow4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_data.sample(False, 0.0003).collect() #first look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Auxilliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_object(x, formatting, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Determine if an object can be converted to a datetime object given an inputted format and if it is the case\n",
    "    if this object is between two dates.\n",
    "    \n",
    "    @x: a python object\n",
    "    @formatting: the desired datetime format\n",
    "    @min_val: a string in the format %Y-%m that is the minimum of the date range\n",
    "    @max_val: a string in the format %Y-%m that is the maximum of the date range\n",
    "    \"\"\"\n",
    "    try: \n",
    "        return((type(dt.strptime(x, formatting)) is dt) and (\n",
    "            dt.strptime(min_val,'%Y-%m-%d') <= dt.strptime(x, formatting) < dt.strptime(max_val,'%Y-%m-%d')))\n",
    "    except:\n",
    "        return(False)\n",
    "\n",
    "\n",
    "    \n",
    "def integer(x, values):\n",
    "    \"\"\"\n",
    "    Determine if the input is an integer and if it is, determine if it belongs to some specified values.\n",
    "    \n",
    "    @x: A python object\n",
    "    @values: A list of excepted values or None if we only want to determine if the input is an integer and\n",
    "    nothing else.\n",
    "    \"\"\"\n",
    "    if isinstance(values, list):\n",
    "        try:\n",
    "            return(int(x) in values)\n",
    "        except:\n",
    "            return(False)\n",
    "        \n",
    "        \n",
    "    elif values is None:\n",
    "        try:\n",
    "            return(isinstance(int(x), int))\n",
    "        except:\n",
    "            return(False)\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        try: \n",
    "            return(int(x) == values)\n",
    "        except:\n",
    "            return(False)\n",
    "\n",
    "        \n",
    "\n",
    "def limits(x, limit_min, limit_max):\n",
    "    \"\"\"\n",
    "    Given an input x, it determine if x fits between two numerical values (both not comprised).\n",
    "    \n",
    "    @x: a python object\n",
    "    @limit_min: a float that represents the first value (lower limit) that we should not expect from x\n",
    "    @limit_max: a float that represents the first value (upper limit) that we should not expect from x\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return(limit_min < float(x) < limit_max)\n",
    "    except:\n",
    "        return(False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def visu_dirty(schemas, total_data, validation_func):\n",
    "    \"\"\"\n",
    "    This function prints for every column present in a grouped dataset all the possible\n",
    "    broken values that the values in this column can take and count the number of time a particular value\n",
    "    has been observed. The validity of a value is defined by a validation function.\n",
    "    \n",
    "    The function is mainly use to see which columns have broken value & what values they take when they\n",
    "    are broken and how often do they take each value.\n",
    "    \n",
    "    @schemas: is a list of tuples where the first value of the tuple is a date & the second is a schema.\n",
    "    It is the product of the diff_schema function.\n",
    "    @total_data: is an rdd where each \"row\" is a list or a tuple. It is intended to be used on the merged data\n",
    "    where all the rows share the same schema (aka the last schema)\n",
    "    @validation func: is a function object which output an iterable of two elements, the first one is a list\n",
    "    of conditions. The number of those conditions is the same as the number of column in the total_data. \n",
    "    Its second element is a \"general\" rule which is true if all the element in the first element is true.\n",
    "    (it check the overall validity of the full row.)\n",
    "    \"\"\"\n",
    "    last_schema = schemas[-1][1].split(\",\")\n",
    "    dirty_records = total_data.filter(lambda x: not (validation_func(x)[1]))\n",
    "    for idx, col_name in enumerate(last_schema):\n",
    "        \n",
    "        print(col_name+\":\", \"\\n\", dirty_records.map(lambda x : (x[idx], not (validation_func(x)[0][idx]))).\n",
    "        filter(lambda x: x[1]).map(lambda x: (x[0], 1)).reduceByKey(lambda x,y : x + y).collect())\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "def visu_all(schemas, total_data, idxs):\n",
    "    \"\"\"\n",
    "    This function prints the unique values along with the number of observations for each \n",
    "    specificed columns of an rdd.\n",
    "    \n",
    "    This function is mainly used after the visu_dirty function on the columns that have been identified\n",
    "    with broken data.\n",
    "    \n",
    "    @schemas: is the result of diff_schema on a particular data\n",
    "    @total_data: is a rdd where each row is an iterable.\n",
    "    @idx: is the indexes of the columns we want to look at in this particular dataset.\n",
    "    \"\"\"\n",
    "    last_schema = schemas[-1][1].split(\",\")\n",
    "    for idx in idxs:\n",
    "        \n",
    "        print(last_schema[idx]+\":\", \"\\n\", total_data.map(lambda x: (x[idx], 1)).\n",
    "              reduceByKey(lambda x,y : x + y).take(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Cleaning of FHV data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.21 Analysis of valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the different schemas\n",
    "\n",
    "print(unique_schema_fhv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.22 Validity rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the validity rules based on the data dictionnary\n",
    "\n",
    "def validation_fhv(x):\n",
    "    dispatching_base_num,pickup_datetime,dropoff_datetime,pulocationid,dolocationid,sr_flag = x\n",
    "    zone_id = np.arange(1, 264).astype(str)\n",
    "    \n",
    "    valid0 = (integer(dispatching_base_num[1:], None)) and (len(dispatching_base_num)== 6) and (\n",
    "        dispatching_base_num[0] == 'B')\n",
    "    \n",
    "    valid1 = time_object(pickup_datetime,'%Y-%m-%d %H:%M:%S', \"2014-12-30\", \"2020-07-02\" )\n",
    "    \n",
    "    valid2 = (time_object(dropoff_datetime,'%Y-%m-%d %H:%M:%S',  \"2014-12-30\", \"2020-07-02\")) or (\n",
    "        (dropoff_datetime == \"\") and (time_object(pickup_datetime,'%Y-%m-%d %H:%M:%S', \"2014-12-30\", \"2017-01-01\")) )\n",
    "    \n",
    "    valid3 = pulocationid in zone_id\n",
    "    \n",
    "    valid4 = (dolocationid in zone_id) or (\n",
    "        (dolocationid == \"\") and (time_object(pickup_datetime,'%Y-%m-%d %H:%M:%S', \"2014-12-30\", \"2017-01-01\")))\n",
    "    \n",
    "    valid5 = (sr_flag in [\"0\", \"1\"])  or (\n",
    "        (sr_flag == \"\") and (time_object(pickup_datetime,'%Y-%m-%d %H:%M:%S', \"2014-12-30\", \"2017-07-01\")))\n",
    "   \n",
    "    out = [valid0, valid1, valid2, valid3, valid4, valid5]\n",
    "   \n",
    "    rules = (valid0 and valid1 and valid2 and valid3 and valid4 and valid5)\n",
    "    return(out, rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Identifying & repairing dirty records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu_dirty(unique_schema_fhv, fhv_data, validation_fhv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu_all(unique_schema_fhv, fhv_data, [5]) #we only check those with an issue here #it also offers a count\n",
    "#of all the rows in the rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_sr(x):\n",
    "    \n",
    "    if x == \"0\":\n",
    "        return(x)\n",
    "    elif (x > \"0\") and (integer(x, None)):\n",
    "        return(\"1\")\n",
    "    else:\n",
    "        return(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repaired_fhv = (fhv_data.\n",
    "                  map(lambda x: [x[0].strip(), x[1], x[2], x[3], x[4], change_sr(x[5])]))\n",
    "\n",
    "visu_dirty(unique_schema_fhv, repaired_fhv, validation_fhv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fhv = repaired_fhv.filter(lambda x: validation_fhv(x))\n",
    "final_fhv.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Cleaning FHVHV data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.31 Analysis of valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2019-02', 'hvfhs_license_num,dispatching_base_num,pickup_datetime,dropoff_datetime,pulocationid,dolocationid,sr_flag')]\n"
     ]
    }
   ],
   "source": [
    "#loading the different schemas\n",
    "\n",
    "print(unique_schema_fhvhv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.32 Validity rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def license_number(proposed_number):\n",
    "    available_numbers = ['HV0002','HV0003','HV0004','HV0005']\n",
    "    return(proposed_number in available_numbers)\n",
    "\n",
    " \n",
    "\n",
    "def validation_fhvhv(x):\n",
    "    hvfhs_license_num,dispatching_base_num,pickup_datetime,dropoff_datetime,pulocationid,dolocationid,sr_flag = x\n",
    "    zone_id = np.arange(1, 264).astype(str)\n",
    "    valid0 = license_number(hvfhs_license_num)\n",
    "    valid1 = (integer(dispatching_base_num[1:], None)) and (len(dispatching_base_num)== 6) and (dispatching_base_num[0] == 'B')\n",
    "    valid2 = time_object(pickup_datetime,'%Y-%m-%d %H:%M:%S', \"2018-12-30\", \"2020-07-01\" )\n",
    "    valid3 = time_object(dropoff_datetime,'%Y-%m-%d %H:%M:%S',  \"2018-12-30\", \"2020-07-02\")\n",
    "    valid4 = pulocationid in zone_id\n",
    "    valid5 = dolocationid in zone_id\n",
    "    valid6 = (sr_flag == \"1\") or (sr_flag == \"0\")\n",
    " \n",
    "    out = [valid0, valid1, valid2, valid3, valid4, valid5, valid6]\n",
    "    \n",
    "    rules = (valid0 and valid1 and valid2 and valid3 and valid4 and valid5 and valid6)\n",
    "    \n",
    "    return(out, rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.33 Identifying & repairing dirty records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvfhs_license_num: \n",
      " []\n",
      "dispatching_base_num: \n",
      " [('', 3)]\n",
      "pickup_datetime: \n",
      " []\n",
      "dropoff_datetime: \n",
      " []\n",
      "pulocationid: \n",
      " [('265', 20)]\n",
      "dolocationid: \n",
      " [('265', 9794), ('264', 2)]\n",
      "sr_flag: \n",
      " []\n"
     ]
    }
   ],
   "source": [
    "visu_dirty(unique_schema_fhvhv, fhvhv_data, validation_fhvhv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321819"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = fhvhv_data.count()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot fix any of the data above. It will lead to the removal of 10 000 rows on 320 000 in total, which would lead to the removal of 3% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312006"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_fhvhv = fhvhv_data.filter(lambda x: (validation_fhvhv(x)[1]))\n",
    "final_fhvhv.count() #we get indeed 310 000 rows after cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Cleaning of GREEN dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.41 Analysis of valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2013-08', 'vendorid,lpep_pickup_datetime,lpep_dropoff_datetime,store_and_fwd_flag,ratecodeid,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,total_amount,payment_type,trip_type'), ('2015-01', 'vendorid,lpep_pickup_datetime,lpep_dropoff_datetime,store_and_fwd_flag,ratecodeid,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type'), ('2016-07', 'vendorid,lpep_pickup_datetime,lpep_dropoff_datetime,store_and_fwd_flag,ratecodeid,pulocationid,dolocationid,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type'), ('2019-01', 'vendorid,lpep_pickup_datetime,lpep_dropoff_datetime,store_and_fwd_flag,ratecodeid,pulocationid,dolocationid,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type,congestion_surcharge')]\n"
     ]
    }
   ],
   "source": [
    "print(unique_schema_green)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.42 Validity rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validation_green(x):\n",
    "    \"\"\" \n",
    "    This function returns True if all the dimension of a record is conformed with the data definitions\n",
    "    of the green data. \n",
    "    \n",
    "    @x is a list\n",
    "    \"\"\"\n",
    "    vendorid, putime, dotime, store_flag, ratecodeid, puloc, doloc, nbr_pass, distance, fare_mt, extra, mta, tip, tolls, ehail, improve, total, pay_type, trip_type, congestion = x\n",
    "    zone_id = np.arange(1, 264).astype(str)\n",
    "    valid0 = integer(vendorid, [1,2])\n",
    "    valid1 = time_object(putime,'%Y-%m-%d %H:%M:%S', \"2013-07-30\", \"2020-07-01\" )\n",
    "    valid2 = time_object(dotime,'%Y-%m-%d %H:%M:%S',  \"2013-07-30\", \"2020-07-02\")\n",
    "    valid3 = (store_flag == \"Y\") or (store_flag == \"N\")\n",
    "    valid4 = integer(ratecodeid, [1,2,3,4,5,6])\n",
    "    valid5 = puloc in zone_id\n",
    "    valid6 = doloc in zone_id\n",
    "    valid7 = (integer(nbr_pass, None)) and (limits(x[7], 0, 10)) \n",
    "    valid8 = limits(distance, 0, 60)\n",
    "    valid9 = limits(fare_mt, -0.1, 501)\n",
    "    valid10 = extra in [\"0\", \"0.5\", \"1\"] #extra, je sais pas, je sais que \n",
    "    valid11 = mta in [\"0\", \"0.5\"]\n",
    "    valid12 = limits(tip, -0.001, 100)\n",
    "    valid13 = limits(tolls, -0.001, 50)\n",
    "    valid14 = limits(ehail, -0.001, 5)\n",
    "    valid15 = (improve == \"0.3\") or ((improve == \"0\") and (time_object(putime,'%Y-%m-%d %H:%M:%S', \"2013-07-30\", \"2015-01-01\")))\n",
    "    valid16 = limits(total, -0.1, 750)\n",
    "    valid17 = integer(pay_type, [1,2,3,4,5,6])\n",
    "    valid18 = integer(trip_type, [1,2])\n",
    "    valid19 = (limits(congestion, -0.0001, 5))\n",
    "    \n",
    "    out = [valid0, valid1, valid2, valid3, valid4, valid5, valid6, valid7, valid8, valid9, valid10, valid11, valid12, valid13, valid14, valid15, valid16, valid17, valid18, valid19]\n",
    "    \n",
    "    rules = (valid0 and valid1 and valid2 and valid3 and valid4 and valid5 and valid6 and valid7 and valid8\n",
    "            and valid9 and valid10 and valid11 and valid12 and valid13 and valid14 and valid15 and valid16 and valid17 and \n",
    "            valid18 and valid19)\n",
    "    \n",
    "    return(out, rules)\n",
    "\n",
    "\n",
    "a = \"2,2020-06-09 09:23:05,2020-06-09 09:33:07,N,1,74,151,1,2.01,9,0,0.5,0,0,,0.3,9.8,2,1,0\".split(\",\")\n",
    "validation_green(a)[0]\n",
    "\n",
    "\n",
    "a = \"2,2020-06-09 09:23:05,2020-06-09 09:33:07,N,1,74,151,1,2.01,9,0,0.5,0,0,,0.3,9.8,2,1,0\".split(\",\")\n",
    "validation_green(a)[0]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.43 Identifying dirty records & repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vendorid: \n",
      " [('', 512)]\n",
      "lpep_pickup_datetime: \n",
      " [('2009-01-01 01:41:47', 1)]\n",
      "lpep_dropoff_datetime: \n",
      " [('2009-01-01 02:24:53', 1)]\n",
      "store_and_fwd_flag: \n",
      " [('', 512)]\n",
      "ratecodeid: \n",
      " [('', 512), ('99', 1)]\n",
      "pulocationid: \n",
      " [('', 209), ('265', 24), ('264', 90), ('0', 1)]\n",
      "dolocationid: \n",
      " [('', 392), ('265', 114), ('264', 89), ('0', 32)]\n",
      "passenger_count: \n",
      " [('', 512), ('0', 72)]\n",
      "trip_distance: \n",
      " [('-15.34', 1), ('.00', 2166), ('56.35', 1)]\n",
      "fare_amount: \n",
      " [('-140', 1), ('200', 2), ('499', 1), ('190', 1), ('131.5', 1), ('170', 1), ('150', 8), ('114', 2), ('130', 1), ('109', 1), ('165', 1), ('108.5', 1), ('-3', 33), ('145.5', 1), ('400', 2), ('103.5', 1), ('419.88', 1), ('-12', 1), ('-2.5', 86), ('1200.5', 1), ('115.5', 1), ('300', 1), ('-25', 2), ('-52', 8), ('-5', 21), ('-30', 2), ('124.5', 2), ('169.5', 1), ('125', 3), ('110.5', 1), ('495', 1), ('-4', 22), ('-32', 1), ('107', 1), ('-7.5', 3), ('160', 2), ('-22.3', 1), ('160.5', 1), ('-5.5', 14), ('-3.5', 23), ('112.5', 1), ('100.5', 3), ('134.5', 1), ('189', 1), ('106.5', 1), ('-10', 1), ('106', 1), ('-4.5', 25), ('178.5', 1), ('147', 1), ('-6.5', 1), ('155', 1), ('175', 1), ('500', 1), ('115', 1), ('-150', 1), ('-50', 1), ('113.5', 1), ('-11', 1), ('-20', 2), ('129.98', 1), ('250', 1), ('-6', 5), ('120', 5), ('100', 7), ('140', 1)]\n",
      "extra: \n",
      " [('34.67', 1), ('2.75', 387), ('16', 1), ('0.2', 2), ('4.5', 26), ('3.75', 28), ('0.58', 1), ('3.25', 38), ('-0.5', 95), ('-1', 41), ('-4.5', 1), ('25', 1), ('8', 1), ('5.5', 12)]\n",
      "mta_tax: \n",
      " [('-0.5', 243)]\n",
      "tip_amount: \n",
      " [('70', 1), ('31', 1), ('23', 1), ('41.5', 1), ('39', 1), ('53', 1), ('35.2', 1), ('27.32', 1), ('26.69', 1), ('78', 1), ('20.8', 1), ('26.2', 1), ('51', 1), ('28', 2), ('21.4', 1), ('90', 1), ('22.3', 1), ('75.55', 1), ('24.84', 1), ('30', 7), ('28.5', 1), ('35.8', 1), ('22.29', 1), ('32', 3), ('35.35', 1), ('29', 2), ('76', 1), ('21.25', 1), ('61', 1), ('50', 10), ('68', 1), ('-2.7', 1), ('30.8', 1), ('21', 2), ('24', 1), ('50.16', 1), ('67', 1), ('33', 3), ('24.27', 1), ('60', 1), ('43', 1), ('45', 6), ('35', 2), ('20.45', 1), ('26.34', 1), ('20.2', 2), ('50.5', 1), ('20.4', 1), ('62.33', 1), ('58.56', 1), ('97', 1), ('55', 1), ('60.1', 1), ('25.9', 1), ('-2.2', 1), ('24.26', 1), ('41.7', 1), ('-1', 1), ('20.16', 1), ('40', 2), ('26.5', 1), ('25', 5), ('29.08', 1), ('21.26', 1), ('37', 2), ('21.33', 1), ('22.56', 1), ('21.16', 1), ('26.11', 1), ('21.5', 1), ('20.26', 1), ('20.12', 1), ('96', 1), ('27.5', 1), ('99', 1), ('20', 29), ('46.7', 1), ('120', 1), ('21.14', 1)]\n",
      "tolls_amount: \n",
      " [('23', 1), ('23.04', 1), ('23.75', 1), ('20.83', 1), ('49.02', 1), ('25', 1), ('-15.29', 1), ('21.58', 1), ('22.02', 1), ('22.83', 1), ('20', 1), ('42.35', 1)]\n",
      "ehail_fee: \n",
      " [('', 154014)]\n",
      "improvement_surcharge: \n",
      " [('-0.3', 205), ('0', 3202)]\n",
      "total_amount: \n",
      " [('105.6', 2), ('-5.3', 18), ('107.8', 1), ('145.56', 1), ('109.5', 1), ('200', 1), ('107.76', 1), ('189.8', 1), ('115.8', 1), ('104.06', 1), ('113.3', 2), ('105.69', 1), ('499', 1), ('190', 1), ('142.06', 1), ('105.54', 1), ('172.3', 1), ('195', 1), ('121.56', 1), ('150', 7), ('114.8', 1), ('1202.3', 1), ('114', 1), ('120.96', 1), ('130', 3), ('136.3', 1), ('128.3', 1), ('109', 1), ('165', 1), ('-3', 4), ('100.08', 1), ('145.5', 1), ('102.36', 1), ('108', 1), ('400', 2), ('104.85', 1), ('-20.8', 1), ('419.88', 1), ('156.65', 1), ('127.56', 1), ('594', 1), ('112.81', 1), ('114.14', 1), ('-2.5', 1), ('103.57', 1), ('115.5', 1), ('100.61', 1), ('300', 1), ('-25', 1), ('145.7', 1), ('110.92', 1), ('119.12', 1), ('-5.8', 26), ('106.25', 1), ('-5', 4), ('102', 1), ('220', 1), ('126.96', 1), ('-30', 2), ('132', 1), ('118.56', 1), ('110.76', 1), ('103.58', 1), ('-3.3', 26), ('109.8', 1), ('125', 1), ('110', 2), ('-4.8', 19), ('101.3', 2), ('125.3', 1), ('-9.3', 1), ('113.96', 1), ('-6.3', 16), ('-4.3', 41), ('-32', 1), ('160', 2), ('127.96', 1), ('-22.3', 1), ('-12.3', 1), ('-5.5', 2), ('111.95', 1), ('143.38', 1), ('120.5', 1), ('-3.5', 3), ('145', 1), ('128.9', 1), ('270', 1), ('104.16', 2), ('-140.3', 1), ('113.16', 1), ('108.8', 1), ('100.8', 2), ('-7', 1), ('107.64', 1), ('113.04', 1), ('170.8', 1), ('-7.3', 4), ('117', 1), ('-11.8', 1), ('-10', 1), ('-40.29', 1), ('168.32', 1), ('-4.5', 1), ('-150.5', 1), ('-6.5', 1), ('119.46', 2), ('155', 1), ('175', 1), ('-3.8', 48), ('145.38', 1), ('-8.3', 2), ('-50', 1), ('157.25', 1), ('107.3', 1), ('-6.8', 11), ('-20', 1), ('250.79', 1), ('-52.8', 5), ('123.5', 1), ('129.98', 1), ('250', 1), ('-6', 4), ('142.59', 1), ('-57.3', 1), ('147.8', 1), ('105.8', 1), ('120', 3), ('-52.5', 2), ('104', 1), ('133.44', 1), ('102.79', 1), ('106.87', 1), ('100', 5), ('500.3', 1)]\n",
      "payment_type: \n",
      " [('', 512)]\n",
      "trip_type: \n",
      " [('', 6402)]\n",
      "congestion_surcharge: \n",
      " [('', 1591)]\n"
     ]
    }
   ],
   "source": [
    "#we identify first what are the possible \"dirty values\" that each columns can take\n",
    "visu_dirty(unique_schema_green, green_data, validation_green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154014"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visu_all(unique_schema_green, green_data, [8, 14, 18,19]) #we only check those with an issue here\n",
    "green_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'RDD' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-35265c6e0f5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgreen_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'RDD' object is not iterable"
     ]
    }
   ],
   "source": [
    "for i in green_data:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the only value we can really repair is the trip distance problem & we can also remove the column ehail fee because it is always a missing value. Instead of removing it, we will instead put all its values to 0. We can not unfortunatly fix the issue for trip type & congestion surchage. We are now gonna apply this knowledge to fix partly the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_str(x):\n",
    "    try:\n",
    "        z = str(abs(int(x)))\n",
    "        return(z)\n",
    "    except:\n",
    "        z = str(abs(float(x)))\n",
    "        return(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we essentially drop the columns \"ehail\" & we also drop all rows with missing/non-conform values\n",
    "def abs_str(x):\n",
    "    \"\"\"convert negative values to positive values & then retransform them to a string\"\"\"\n",
    "    try:\n",
    "        z = str(abs(int(x)))\n",
    "        return(z)\n",
    "    except:\n",
    "        z = str(abs(float(x)))\n",
    "        return(z)\n",
    "    \n",
    "    \n",
    "    \n",
    "repaired_green = (green_data.\n",
    "                  map(lambda x: flatten_nan([x[:8], [\"0\" if x[8] == \".00\" else abs_str(x[8])], \n",
    "                                             [abs_str(x[9])] ,[x[10]], [abs_str(x[11])], [abs_str(x[12])], \n",
    "                                            [abs_str(x[13])], [\"0\"] , [abs_str(x[15])], [abs_str(x[16])], x[17:]])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu_dirty(unique_schema_green, repaired_green, validation_green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_green = repaired_green.filter(lambda x: validation_green(x)[1])\n",
    "final_green.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Cleaning of yellow data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.51 Analysis of valid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.52 Validity rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validation_yellow(x):\n",
    "    vendorid, putime, dotime, nbr_pass, distance, ratecodeid, store_flag, puloc, doloc, pay_type,  fare_mt, extra, mta, tip, tolls, improve, total, congestion = x\n",
    "    zone_id = np.arange(1, 264).astype(str)\n",
    "    valid0 = integer(vendorid, [1,2])\n",
    "    valid1 = time_object(putime,'%Y-%m-%d %H:%M:%S', \"2008-12-30\", \"2020-07-02\" )\n",
    "    valid2 = time_object(dotime,'%Y-%m-%d %H:%M:%S',  \"2008-12-30\", \"2020-07-02\")\n",
    "    valid3 = (integer(nbr_pass, None)) and (limits(nbr_pass, 0, 10))\n",
    "    valid4 = limits(distance, 0, 100)\n",
    "    valid5 = integer(ratecodeid, [1,2,3,4,5,6])\n",
    "    valid6 = (store_flag == \"Y\") or (store_flag == \"N\")\n",
    "    valid7 = puloc in zone_id\n",
    "    valid8 = doloc in zone_id\n",
    "    valid9 = pay_type in [\"1\", \"2\", \"3\", \"4\", \"5\",\" 6\"]\n",
    "    valid10 = limits(fare_mt, -0.1 , 100)\n",
    "    valid11 = extra in [\"0\", \"0.5\", \"1\"]\n",
    "    valid12 = mta in [\"0\", \"0.5\"]\n",
    "    valid13 = limits(tip, -0.001, 20)\n",
    "    valid14 = limits(tolls, -0.001, 20)\n",
    "    valid15 = (improve == \"0.3\") or (improve == \"0\" and time_object(putime,'%Y-%m-%d %H:%M:%S', \"2008-12-30\", \"2015-01-01\"))\n",
    "    valid16 = limits(total, -0.1 , 100)\n",
    "    valid17 = (limits(congestion, -0.0001, 5))\n",
    "    \n",
    "    out = [valid0, valid1, valid2, valid3, valid4, valid5, valid6, valid7, valid8, valid9, \n",
    "           valid10, valid11, valid12, valid13, valid14, valid15, valid16, valid17]\n",
    "    \n",
    "    rules = (valid0 and valid1 and valid2 and valid3 and valid4 and valid5 and valid6 and valid7 and valid8\n",
    "            and valid9 and valid10 and valid11 and valid12 and valid13 and valid14 and valid15 and valid16 and \n",
    "             valid17)\n",
    "    \n",
    "    return(out, rules)\n",
    "\n",
    "\n",
    "a = \"1,2020-06-09 09:23:05,2020-06-09 09:33:07,1,10,3,Y,50,50,1,0.5,0,0,0.5,0.5,0.3,5.2,3.9\".split(\",\")\n",
    "not (validation_yellow(a)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.53 Identifying dirty records & repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vendorid: \n",
      " [('', 505), ('CMT', 1011211), ('DDS', 28264), ('3', 8), ('VTS', 1028129), ('4', 1508)]\n",
      "tpep_pickup_datetime: \n",
      " []\n",
      "tpep_dropoff_datetime: \n",
      " []\n",
      "passenger_count: \n",
      " [('', 505), ('0', 12055), ('208', 3)]\n",
      "trip_distance: \n",
      " [('60.259999999999998', 1), ('50.60', 1), ('96.26', 1), ('500.00', 1), ('81.700000000000003', 1), ('62.799999999999997', 2), ('61.200000000000003', 1), ('54.75', 1), ('57.20', 1), ('71.90', 1), ('494.40', 1), ('55.17', 1), ('83.10', 1), ('93.930000000000007', 1), ('70', 1), ('250.10', 1), ('51.95', 1), ('60.200000000000003', 1), ('71.700000000000003', 1), ('50.299999999999997', 1), ('84.5', 1), ('58.700000000000003', 1), ('59', 1), ('55.219999999999999', 1), ('71.07', 1), ('-7.64', 1), ('88.099999999999994', 1), ('0', 13986), ('77.299999999999997', 1), ('59.700000000000003', 1), ('300833.10', 1), ('64', 1), ('57.100000000000001', 1), ('145.90', 1), ('54.70', 1), ('86.00', 1), ('74.629999999999995', 1), ('90', 1), ('50.020000000000003', 1), ('79.299999999999997', 1), ('84.97', 1), ('100', 1), ('61.5', 1), ('60', 1), ('53', 1), ('80', 1), ('51.060000000000002', 1), ('86.36', 1), ('56.20', 1), ('72.299999999999997', 1), ('63.399999999999999', 1), ('62.00', 1), ('150.00', 1), ('60.42', 1), ('64.200000000000003', 2), ('50.509999999999998', 1), ('130.10', 1), ('53.17', 1), ('.00', 7180), ('88.799999999999997', 1), ('79.069999999999993', 1), ('70.25', 1), ('96.400000000000006', 1), ('50.899999999999999', 1), ('50.01', 1), ('89.95', 1), ('57.50', 1), ('300.00', 1), ('-19.37', 1), ('-16.11', 1), ('52.979999999999997', 1), ('50.200000000000003', 1), ('64.64', 1), ('75', 1), ('68.00', 1), ('78.200000000000003', 1), ('60.00', 1), ('54.460000000000001', 1), ('72.799999999999997', 1), ('54.700000000000003', 1), ('50', 4), ('78.540000000000006', 1), ('184.80000000000001', 1), ('56.80', 1), ('76.099999999999994', 1), ('51', 1), ('80.200000000000003', 1), ('71.599999999999994', 1), ('69', 1), ('62.83', 1), ('56.200000000000003', 1), ('51.80', 1), ('52.100000000000001', 1), ('55.11', 1), ('396.30', 1), ('58.70', 1), ('51.70', 1), ('58.640000000000001', 1), ('73.200000000000003', 1), ('51.549999999999997', 1), ('79.400000000000006', 1), ('60.90', 1), ('69.50', 1), ('56', 1), ('202', 1), ('80.400000000000006', 1), ('56.5', 1), ('188.40', 1), ('74.700000000000003', 1), ('73.900000000000006', 1), ('54.79', 1), ('54', 1)]\n",
      "ratecodeid: \n",
      " [('', 342379), ('24', 1), ('35', 4), ('8', 2), ('0', 402), ('128', 1), ('210', 6), ('99', 52), ('16', 4), ('33', 2)]\n",
      "store_and_fwd_flag: \n",
      " [('', 1076215), ('0', 223431), ('1', 3889)]\n"
     ]
    }
   ],
   "source": [
    "visu_dirty(unique_schema_yellow, yellow_data, validation_yellow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think DDS is just another third suppliers of data (that existed before, I would not diregard it for that,\n",
    "#trip distance converted to positive\n",
    "#store  & fwd flag, I should watch the visu_all on it, bu I would assume if missing, it is bc Not forwarded\n",
    "#otherwise It would be weird, also 0 are not forwarded (or atleast the most observed value) & 1 to be forwarded\n",
    "\n",
    "visu_all(unique_schema_yellow, yellow_data, [8, 14, 18,19]) #needs to be adapted to yellow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_to_id(name):\n",
    "    \"\"\"\n",
    "    This function transforms the vendor name to its corresponding ID\"\"\"\n",
    "    x = name.upper()\n",
    "    \n",
    "    if (x == 'CMT') or (x == '1') :\n",
    "        return(\"1\")\n",
    "    \n",
    "    elif (x == 'VTS') or (x == \"2\"):\n",
    "        return(\"2\")\n",
    "    \n",
    "    else:\n",
    "        return(\"\")\n",
    "\n",
    "    \n",
    "\n",
    "def convert_pay_type(pay_type):\n",
    "    x = pay_type.upper()\n",
    "    \n",
    "    if (x == 'CREDIT') or (x == 'CRE') or (x ==  \"1\"):\n",
    "        return('1')\n",
    "    \n",
    "    elif (x == 'CASH') or (x == \"CAS\") or (x == \"2\"):\n",
    "        return('2')\n",
    "    \n",
    "    elif (x == 'NO CHARGE') or (x == 'NO') or (x =='3'):\n",
    "        return('3')\n",
    "    \n",
    "    elif (x == 'DISPUTE') or (x == \"DIS\") or (x == \"4\"):\n",
    "        return('4')\n",
    "    \n",
    "    elif (x == '5'):\n",
    "        return('5')\n",
    "    \n",
    "    elif (x == '6'):\n",
    "        return('6')\n",
    "\n",
    "    else:\n",
    "        return(\"\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repairement of broken data in the vendor_id & pay_type & dropping all the missingg/ill-defined records\n",
    "\n",
    "repaired_yellow = (yellow_data.map\n",
    "                   (lambda x: flatten_nan([[name_to_id(x[0])], x[1:9], [convert_pay_type(x[9])], x[10:]])))\n",
    "\n",
    "visu_dirty(unique_schema_yellow, repaired_yellow, validation_yellow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_yellow = repaired_yellow.filter(lambda x: validation_yellow(x)[1])\n",
    "final_yellow.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Auxillary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def to_month(x, idx_date,  idx):\n",
    "    \"\"\"\n",
    "    This function changes the date from normal frequency to monthly frequency & select only the\n",
    "    columns of interest to solve the particular problem at hand\n",
    "    x is list\n",
    "    idx_date is the index (an integer) in the list x where there is a date object\n",
    "    idx is a list with the indexes of the particular values we want to keep for the analysis\n",
    "    \"\"\"\n",
    "    val = [x[i] for i in idx]\n",
    "    datum = dt.strptime(x[idx_date], '%Y-%m-%d %H:%M:%S')\n",
    "    date_month = dt.strftime(datum, '%Y-%m')\n",
    "    out = (date_month, val)\n",
    "    return(out)\n",
    "\n",
    "b = [\"2020-06-15 09:23:05\", \"truc_imp\", \"pasimp\", \"superimp\", \"notimp\"]\n",
    "to_month(b, 0, [1,3])\n",
    "\n",
    "\n",
    "\n",
    "def to_month(x, idx_date,  idx):\n",
    "    \"\"\"\n",
    "    This function changes the date from normal frequency to monthly frequency & select only the\n",
    "    columns of interest to solve the particular problem at hand\n",
    "    x is list\n",
    "    idx_date is the index (an integer) in the list x where there is a date object\n",
    "    idx is a list with the indexes of the particular values we want to keep for the analysis\n",
    "    \"\"\"\n",
    "    val = [x[i] for i in idx]\n",
    "    datum = dt.strptime(x[idx_date], '%Y-%m-%d %H:%M:%S')\n",
    "    date_month = dt.strftime(datum, '%Y-%m')\n",
    "    out = (date_month, val)\n",
    "    return(out)\n",
    "\n",
    "def to_month(x):\n",
    "    datum = dt.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "    date_month = dt.strftime(datum, '%Y-%m')\n",
    "    return(date_month)\n",
    "def plotting(datas, title, colors = [\"yellow\", \"green\", \"orange\", \"red\"], \n",
    "             labels = [\"Yellow Taxis\", \"Green Taxis\", \"FHV\", \"FHVHV\"], title_position = (0.4, 1.05)\n",
    "            , legend_position = \"lower right\"):\n",
    "    \"\"\"\n",
    "    This function plot time series with some predefined aestetics parameters.\n",
    "    \n",
    "    @datas is a list containing lists of tuples with tuples of the following form: (date, value). \n",
    "    It is important to always put the dataset with the most values first.\n",
    "    @title is the title of the plot\n",
    "    @colors is list containing the color of the different datasets. It must have the same number\n",
    "    of elements as the data arguments (same number of lists)\n",
    "    @labels are the labels of the different series to be printed in the legend of the graph.\n",
    "    It must have the same number of elements as the data arguments (same number of lists).\n",
    "    @title position is a tuple with two elements, the first one specify its position on the x-axis\n",
    "    & the second on the y-axis (represented by floats)\n",
    "    @legend_position specifies the position of the legend position among different possibilities\n",
    "    (e.g. \"upper right\")\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots()\n",
    "    sns.set_style(\"white\")\n",
    "    axs.spines[\"bottom\"].set_color(\"Gray\")\n",
    "    axs.spines[\"left\"].set_color(\"Gray\")\n",
    "    axs.spines[\"bottom\"].set_alpha(0.7)\n",
    "    axs.spines[\"left\"].set_alpha(0.7)\n",
    "    sns.despine()\n",
    "    axs.set_title(title, x = title_position[0], y = title_position[1], \n",
    "                  fontsize = 16, alpha = 0.7, color = \"gray\")\n",
    "    plt.xticks(fontsize = 8, color = \"gray\", alpha = 0.7)\n",
    "    plt.yticks(fontsize = 8, color = \"gray\", alpha = 0.7)\n",
    "    \n",
    "    for data, color, label in zip(datas, colors, labels):\n",
    "        date, count = map(tuple, zip(*data))\n",
    "        axs.plot(date, count, alpha = 0.35, c = color, linewidth = 4, label = label )\n",
    "        \n",
    "    plt.legend(loc = legend_position);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Monthly total number of trips per service\n",
    "\n",
    "grouped_data = (final_yellow, final_green, final_fhv, final_fhvhv)\n",
    "idx_dates = (1,1,1,1) #je mets 1 pcq les autres pas tjrs des do_time\n",
    "idx_nbr_trip = (1, 1, 1, 1)\n",
    "nbr_trip_per_month = []\n",
    "#je pense que dans les to_month, il faut pas mettre les x d'abord et puis prier pour qu'ils acceptent\n",
    "#les valeurs de la loop, pls jesus ==> it does, but you have to provide a lambda function, idk why\n",
    "for data, idx_date, idx_val in zip(grouped_data, idx_dates, idx_nbr_trip):\n",
    "    nbr_trip_per_month.append(data.map(lambda x: to_month(x, idx_date = idx_date, idx = [idx_val]))\n",
    "                              .countByKey().sortByKey().collect())\n",
    "\n",
    "    \n",
    "\n",
    "plotting(nbr_trip_per_month, \"Number of trips per month per service\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Monthly number of trips in Manhattan and Brooklyn grouped per dataset type\n",
    "\n",
    "#we first create a lookup table for the locationID that are in Manhattan & Brooklyn\n",
    "\n",
    "zones = gpd.read_file('taxi_zones.shp') \n",
    "zones = zones.to_crs({'init':'epsg:4326'})\n",
    "man_bro_lookup = list(zones.loc[(zones.borough == \"Manhattan\") | (zones.borough == \"Brooklyn\"), \n",
    "                                \"LocationID\"].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_pulocs = [7, 5, 3, 5]#careful green has one col less so the index can be different than in validation green\n",
    "idx_dolocs = [8, 6, 4, 6] #this is not true anymore, i kept the ehail column, just set it to 0\n",
    "\n",
    "nbr_trip_man_bro = []\n",
    "\n",
    "for data, idx_date, idx_puloc, idx_doloc in zip(grouped_data, idx_dates, idx_pulocs, idx_dolocs):\n",
    "    (nbr_trip_man_bro.append(data.\n",
    "     filter(lambda x: (x[idx_puloc] in man_bro_lookup) and (x[idx_doloc] in man_bro_lookup)). \n",
    "     map(lambda x: to_month(x, idx_date = idx_date, idx = [idx_puloc])).countByKey().sortByKey().collect())) \n",
    "\n",
    "\n",
    "\n",
    "plotting(nbr_trip_man_bro, \"Monthly number of trips in Manhattan and Brooklyn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Monthly total receipts per dataset type\n",
    "\n",
    "grouped_taxi_data = [final_yellow, final_green]\n",
    "idx_dates_small = [1,1]\n",
    "idx_totals = [[13,16], [12,16]]#careful green has one col less so the index can be different than in validation green\n",
    "\n",
    "\n",
    "#maybe we should check what is included in the total for each\n",
    "total_receipts = []\n",
    "\n",
    "for data, idx_date, idx_val in zip(grouped_taxi_data, idx_dates_small, idx_totals):\n",
    "    (total_receipts.append(data.\n",
    "     map(lambda x: to_month(x, idx_date = idx_date, idx = idx_val)).\n",
    "                           map(lambda x: (x[0], float(x[1][1]) - float(x[1][0]))).\n",
    "                           reduceByKey(lambda x,y : x+y).sortByKey().collect()))\n",
    "    \n",
    "plotting(total_receipts, \"Monthly Total Receipts Per Service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. average trip receipts per dataset type\n",
    "\n",
    "\n",
    "#maybe we should check what is included in the total for each\n",
    "avg_receipts = []\n",
    "\n",
    "\n",
    "\n",
    "for data, idx_date, idx_val in zip(grouped_taxi_data, idx_dates_small, idx_totals):\n",
    "    (avg_receipts.append(data.\n",
    "     map(lambda x: to_month(x, idx_date = idx_date, idx = idx_val)).\n",
    "                           map(lambda x: (x[0], float(x[1][1]) - float(x[1][0]))).\n",
    "                           aggregateByKey((0,0), lambda x,y : (x[0] + y, x[1] + 1), lambda x,y : (x[0] + y[0], x[1]+ y[1]))\n",
    "                           mapValues(lambda x : x[0]/x[1].sortByKey().collect()))\n",
    "    \n",
    "plotting(avg_receipts, \"Monthly Average Receipts Per Trip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 Average cost per in-progress-minute \n",
    "\n",
    "def cost_per_min(x):\n",
    "    cost = float(x[1][3]) - float(x[1][2])\n",
    "    duration = dt.strptime(x[1][1], \"%Y-%m-%d %H:%M:%S\") - dt.strptime(x[1][0], \"%Y-%m-%d %H:%M:%S\")\n",
    "    duration_in_minutes = duration.days * 60 * 24 + duration.seconds/ 60\n",
    "    try:\n",
    "        cost_min = cost/ duration_in_minutes #if duration is 0\n",
    "    except:\n",
    "        cost_min = \"div_by_0\"\n",
    "    return((x[0], cost_min))\n",
    "\n",
    "\n",
    "def agg_cost_min(x, y):\n",
    "    \n",
    "    if (y == \"div_by_0\"):\n",
    "        out = (x[0], x[1])\n",
    "        return(out)\n",
    "    else :\n",
    "        out = (x[0] + y, x[1] + 1)\n",
    "        return(out)\n",
    "    \n",
    "\n",
    "\n",
    "avg_cost_per_min = []\n",
    "idx_cost_min = [[1,2,13,16], [1,2, 12,16]]\n",
    "\n",
    "\n",
    "for data, idx_date, idx_val in zip(grouped_taxi_data, idx_dates_small, idx_cost_min):\n",
    "    (avg_cost_per_min.append(data.\n",
    "     map(lambda x: to_month(x, idx_date = idx_date, idx = idx_val)).\n",
    "                           map(cost_per_min).\n",
    "                           aggregateByKey((0,0), agg_cost_min, lambda x,y : (x[0] + y[0], x[1]+ y[1]))\n",
    "                           mapValues(lambda x : x[0]/x[1].sortByKey().collect()))\n",
    "    \n",
    "plotting(avg_cost_per_min, \"Monthly Average Cost Per Minute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 Average tip per trip\n",
    "\n",
    "avg_tips = []\n",
    "\n",
    "idx_tips = [13, 12]\n",
    "\n",
    "for data, idx_date, idx_val in zip(grouped_taxi_data, idx_dates_small, idx_tips):\n",
    "    (avg_receipts.append(data.\n",
    "     map(lambda x: to_month(x, idx_date = idx_date, idx = [idx_val])).\n",
    "                           map(lambda x: (x[0], float(x[1][0])))).\n",
    "                           aggregateByKey((0,0), lambda x,y : (x[0] + y, x[1] + 1), lambda x,y : (x[0] + y[0], x[1]+ y[1]))\n",
    "                           mapValues(lambda x : x[0]/x[1].sortByKey().collect()))\n",
    "    \n",
    "plotting(avg_receipts, \"Monthly Average Tips Per Trip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 Median Monthly  trip speed grouped per service & borough (il faut aussi la locationid)\n",
    "\n",
    "idx_dist_time = [[1, 2, 4, 7, 8], [1, 2, 8, 5, 6]] #in order pu_time, do_time, distance, pu_id, do_id\n",
    "\n",
    "#how to do the borough thing? if both puloc & doloc are in the borough? donc first filtrer que les intraborough?\n",
    "\n",
    "#map(if (name of borough x & y) in list[id in borough] (list id in borough à faire (tu fais un for in unique_value et\n",
    "#puis tu automatises)), else (donc ceux qui sont pas dans les 2), puis filter if diff de la valeur que tu \n",
    "#mets dans le else, puis tu fais un key rdd (où la key est month (reconverti en str) + borough)\n",
    "#for the median there is a function that can help us (which approximate basically)\n",
    "#then I will have for one of the dataset, all the month with all the neighboorhood\n",
    "#then group by neighboorhood, cut that big rdd into smaller one based on that neighboorhood (so you \n",
    "# remove the col neighboorhood, bc it is in the name now, sortby date, & then you plot (date & median) that by facetgrid\n",
    "#so basically, all that concerns the yellow taxi are on graph, & those on the green are on another one\n",
    "\n",
    "\n",
    "#je pense que pour le plot, il faut faire le facet_grid this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_boro(x, y,  zones_id):\n",
    "        borough1 = zones_id.loc[int(x) == (zones_id.LocationID)].borough.iloc[0]\n",
    "        borough2 = zones_id.loc[int(y) == (zones_id.LocationID)].borough.iloc[0]\n",
    "        if borough1 == borough2:\n",
    "            return(borough1)\n",
    "        \n",
    "        else: \n",
    "            return(\"remove\")\n",
    "    \n",
    "\n",
    "def time_diff(x, y):\n",
    "    \"\"\" Gives the time difference between two datetime object in hours\"\"\"\n",
    "    diff = dt.strptime(x, \"%Y-%m-%d %H:%M:%S\") - dt.strptime(y, \"%Y-%m-%d %H:%M:%S\")\n",
    "    diff_in_hour = diff.days *24 + diff.seconds/3600\n",
    "    return(diff_in_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_id = zones[[\"LocationID\", \"borough\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_dist_time = [[1,2,4,7,8], [1,2,8,5,6]] #pu_time, do_time, dist, pu_id, do_id\n",
    "\n",
    "yellow_neigh = (final_yellow.filter(lambda x: to_month(x, idx_small_dates[0], idx = idx_dist_time[0])).\n",
    "                map(lambda x: (x[0], [ time_diff(x[1][1], x[1][0]), float(x[1][3]), finding_boro(x[1][3], x[1][4], zones_id)])).\n",
    "                filter(lambda x: x[1][2] != \"remove\" ).map(lambda x: (x[0],  x[1][2], (x[1][1])/ (x[1][0]))).\n",
    "                toDF([\"date\", \"neighborhood\", \"speed\"]))\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "sns.set_style(\"white\")\n",
    "axs.spines[\"bottom\"].set_color(\"Gray\")\n",
    "axs.spines[\"left\"].set_color(\"Gray\")\n",
    "axs.spines[\"bottom\"].set_alpha(0.7)\n",
    "axs.spines[\"left\"].set_alpha(0.7)\n",
    "sns.despine()\n",
    "axs.set_title(\"Median Trip Speed Per Neighborhood \\nof the Yellow Taxis\", x = 0.4, y = 1.05,\n",
    "                 fontsize = 16, alpha = 0.7, color = \"gray\")\n",
    "plt.xticks(fontsize = 8, color = \"gray\", alpha = 0.7)\n",
    "plt.yticks(fontsize = 8, color = \"gray\", alpha = 0.7)\n",
    "\n",
    "\n",
    "colors = [\"yellow\", \"green\", \"orange\", \"red\", \"brown\", \"salmon\"]\n",
    "\n",
    "for i, color  in zip(zones_id.borough.unique(), colors):\n",
    "    (yellow_neigh[yellow_neigh.neighborhood == i].grouby([\"date\"]).\n",
    "     approxQuantile(\"speed\", [0.5], 0.5).toPandas().sortby(\"date\").\n",
    "     plot(x = \"date\", y = \"speed\", alpha = 0.35, c = color,  label = i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_dist_time = [[1,2,4,7,8], [1,2,8,5,6]] #pu_time, do_time, dist, pu_id, do_id\n",
    "\n",
    "green_neigh = (final_green.filter(lambda x: to_month(x, idx_small_dates[1], idx = idx_dist_time[1])).\n",
    "                map(lambda x: (x[0], [ time_diff(x[1][1], x[1][0]), float(x[1][3]), finding_boro(x[1][3], x[1][4], zones_id)])).\n",
    "                filter(lambda x: x[1][2] != \"remove\" ).map(lambda x: (x[0],  x[1][2], (x[1][1])/ (x[1][0]))).\n",
    "                toDF([\"date\", \"neighborhood\", \"speed\"]))\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "sns.set_style(\"white\")\n",
    "axs.spines[\"bottom\"].set_color(\"Gray\")\n",
    "axs.spines[\"left\"].set_color(\"Gray\")\n",
    "axs.spines[\"bottom\"].set_alpha(0.7)\n",
    "axs.spines[\"left\"].set_alpha(0.7)\n",
    "sns.despine()\n",
    "axs.set_title(\"Median Trip Speed Per Neighborhood \\nof the Green Taxis\", x = 0.4, y = 1.05,\n",
    "                 fontsize = 16, alpha = 0.7, color = \"gray\")\n",
    "plt.xticks(fontsize = 8, color = \"gray\", alpha = 0.7)\n",
    "plt.yticks(fontsize = 8, color = \"gray\", alpha = 0.7)\n",
    "\n",
    "\n",
    "colors = [\"yellow\", \"green\", \"orange\", \"red\", \"brown\", \"salmon\"]\n",
    "\n",
    "for i, color  in zip(zones_id.borough.unique(), colors):\n",
    "    (green_neigh[green_neigh.neighborhood == i].grouby([\"date\"]). #j'ai peur que certains n'ait aucun trip ds certains neighboorhood\n",
    "     approxQuantile(\"speed\", [0.5], 0.5).toPandas().sortby(\"date\").\n",
    "     plot(x = \"date\", y = \"speed\", alpha = 0.35, c = color,  label = i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "il faudrait réflechir au potentiel bug, genre division par 0 et d'autres choses de  genre!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
